\documentclass{article}
\usepackage{amsmath, amsfonts}
\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{paralist}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{geometry}
\geometry{margin=1in}
\begin{document}
\title{Progress Report}
\author{Yige Hu and Zhiting Zhu}
\date{}
\maketitle

% declaration of the new block
\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parfor}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end\ parfor}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\algnewcommand\algorithmicinput{\textbf{INPUT:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}

\section{Sequantial Algorithm}
K-means clustering is a NP-hard problem in general Euclidean space d~\cite{k-means-euclidean}
and even for instances in a plane~\cite{k-means-plane}. The sequantial
algorithm shown here is a heuristic algorithm which does not guarantee
to find a global optimal. 
\begin{algorithm}
  \caption{Sequential k-means clustering} \label{seq}
  \begin{algorithmic}[1]
    \INPUT $K$: Number of clusters; $N$: number of d-dimentional data points; $p$: data points. 
    \Function{seq\_k-means}{$p, N, K$}
    \State Randomly generate $K$ points as cluster centroids $c[]$
    \While {!termination\_condition }
    \State Assign each point to the nearest cluster centroid
    \State Recompute the new cluster centroids
    \EndWhile
    \EndFunction
  \end{algorithmic}
\end{algorithm}
Suppose m is the number of
iteration run in this algorithm. The complexity of this algorithm is
O(Nkdm). 

\section{Parallel k-means clustering}
The obvious way to parallelize the algorithm is to parallelize the
membership assignment part and recompute the new centroid. 
\begin{algorithm}
  \caption{Parallel k-means clustering} \label{par}
  \begin{algorithmic}[1]
    \INPUT $K$: Number of clusters; $N$: number of d-dimentional data points; $p$: data points.
    \Function{par\_k-means}{$p, N, K$} \label{alg:p}
%    \State Partition N data objects evenly among all threads
    \State Randomly choose $K$ points as cluster centroids $c[]$
    \While {! termination\_condition}
    \ParFor {i = 1..N}
    \For {j = 1..K}
    \State Compute distance between point $p(i)$ and centroid $c(j)$
    \EndFor
    \State Find the nearest centroid $c_{nearest}$ for $p(i)$
    \State Change membership of $p(i)$ to the cluster with $c_{nearest}$
    \State Accumulate $p(i)$'s coordinates to the cluster's new centroid
    \EndParFor
    \State Compute new $c[]$: divide the accumulated coords by num\_points
    \State Recalculate termination condition
    \EndWhile
    \EndFunction  
  \end{algorithmic}
\end{algorithm}

\section{Implementation}
We plan to implement this algorithm using CUDA on GPU. If we have
time, we may consider a hybrid implementation with MPI, OpenMP
and CUDA. During implementation, we need to consider serveral
GPU-related optimization such as control divergence, coalesced memory
access and use of shared/constant memory.  

\section{Related Work}
We found two CUDA implementations of parallel k-means
algorithm~\cite{serban-kmeans, gpuminer}. We plan to compare their
implementation with ours and also compare the result with other
MPI/OpenMP implementatoins.  

\section{Experiments Plan}
We plan to test our implementation on TACC Lonestar GPU node. We plan
to test our implementation with more than 600,000 points each with more
than 40 dimention each vector and in total more than 120 centroids. The
ultimate limitation is the size of GPU memory. If
we can use GPU memory more efficiently, we might be able to solve
larger problem size. 

For correctness testing,
we will compare the running result with a known correct
implementation. We will test our
implementation with one CUDA thread. But its performance is
incomperable with single core CPU implementation as GPU runs much
slower than CPU. For weak scaling, we will fix data points assigned to
each CUDA block, increase number of CUDA blocks with increase of
input size and measure total execution time and total execution time taken
on GPU calculation. For strong scaling, we will fix the input size,
increase the number of CUDA blocks and measure total execution
time and total execution time taken on GPU. 

\bibliographystyle{acm}
\bibliography{bibliography}
\end{document}
