\documentclass{article}
\usepackage{amsmath, amsfonts}
\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{paralist}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{geometry}
\geometry{margin=1in}
\begin{document}
\title{Progress Report}
\author{Yige Hu and Zhiting Zhu}
\date{}
\maketitle

% declaration of the new block
\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parfor}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end\ parfor}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\algnewcommand\algorithmicinput{\textbf{INPUT:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}

\section{Sequantial Algorithm}
K-means clustering is an NP-hard problem in general Euclidean space 
$\mathbb{R}^d$~\cite{k-means-euclidean}
and even for instances on a plane~\cite{k-means-plane}. The sequantial
algorithm below is a heuristic algorithm which does not guarantee
a global optimal. 

\begin{algorithm}
  \caption{Sequential k-means clustering} \label{seq}
  \begin{algorithmic}[1]
    \INPUT $K$: Number of clusters; $N$: number of d-dimentional data points; $p$: data points. 
    \Function{seq\_k-means}{$p, N, K$}
    \State Randomly generate $K$ points as cluster centroids $c[]$
    \While {!termination\_condition }
    \State Assign each point to the nearest cluster centroid
    \State Recompute the new cluster centroids
    \EndWhile
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\vspace{5mm}
\noindent
Suppose m is the number of iterations in a run. \\
The complexity of this algorithm is O(NKdm). 

\section{Parallel k-means clustering}
An obvious way to parallelize the algorithm is to parallelize the
parts for membership assignment and recomputation on new centroids.
\begin{algorithm}
  \caption{Parallel k-means clustering} \label{par}
  \begin{algorithmic}[1]
    \INPUT $K$: Number of clusters; $N$: number of d-dimentional data points; $p$: data points.
    \Function{par\_k-means}{$p, N, K$} \label{alg:p}
%    \State Partition N data objects evenly among all threads
    \State Randomly choose $K$ points as cluster centroids $c[]$
    \While {! termination\_condition}
    \ParFor {i = 1..N}
    \For {j = 1..K}
    \State Compute distance between point $p(i)$ and centroid $c(j)$
    \EndFor
    \State Find the nearest centroid $c_{nearest}$ for $p(i)$
    \State Change membership of $p(i)$ to the cluster with $c_{nearest}$
    \State Accumulate $p(i)$'s coordinates to the cluster's new centroid
    \EndParFor
    \State Compute new $c[]$: divide the accumulated coords by num\_points
    \State Recalculate termination condition
    \EndWhile
    \EndFunction  
  \end{algorithmic}
\end{algorithm}

\vspace{5mm}
\noindent
Suppose m is the number of iterations in a run. \\
The work-depth model for this algorithm is: Work = O(NKdm), Depth = O(Kdm).

\section{Implementation}
We plan to implement this algorithm on NVidia GPUs with CUDA platform. If we have
enough time, we may consider a hybrid implementation with MPI, OpenMP
and CUDA. For GPU code, we need also consider serveral
hardware-related optimizations, such as coalesced memory
access, shared/constant memory usage and execution path divergence control.

\section{Related Work}
We found two open-souce CUDA implementations of parallel k-means
algorithm~\cite{serban-kmeans, gpuminer} on the Internet. We plan to compare
our performance with their implementation. We will also make comparison with 
other open-souce MPI/OpenMP implementatoins.

\section{Experiments Plan}
We plan to test our implementation on the TACC Lonestar GPU node. 
The dataset should scale to more than 600,000 points, each with 
40 dimentions or more, and to be clustered into 120 centroids at least. 
The scalability limitation will mainly be caused by GPU memory size. If
we can use memory in a more efficient way, we might be able to do
computation on an even larger problem size. 

We sanity check output correctness by comparing run time results with a known 
previous implementation. 
We will first test our implementation and compare its single-core performance with 
a CPU core, under the same input configuration. We expect 10x--50x speed-up 
over a single CPU core with variant input configurations.
To get weak scalability, we fix the number of points assigned to
each CUDA thread block, and measure execution time with an increasing number 
of thread blocks. Thus, in this case the total input size is propotional to
the number of thread blocks.
While for strong scalability, we fix the total input size and increase the 
number of thread blocks. 
In both cases, we measure total execution time as well as time consumed by code
on GPU, in order to do fair comparison with other implementations.
We use \textbf{gettimeofday()} to measure accurately the run time.
We might also calculate \textbf{\textit{GFlops}/\textit{s}} as an indicator for GPU
computation efficiency.

\bibliographystyle{acm}
\bibliography{bibliography}
\end{document}
