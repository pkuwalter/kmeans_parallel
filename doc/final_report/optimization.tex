\section{Architecture-related optimizations}
\label{s:optimization}

When tuning the program performance, we use several
GPU architecture-related optimizations. Especially, coalesced memory access 
benefits the most, followed by the usage of high-speed memory regions and
prefetching.

\subsection{CUDA memory hierachy}

CUDA threads can access data from several different memory regions in GPU. Each 
thread has its own local and private memory. Each \TB has its own shared
memory regions that is accessible to all threads inside the \TB. The
global memory is large and shared by all the threads. ~\cite{cuda-program-guide}.

Per-thread-accessible registers has negligible read/write overhead compared to 
other kinds of memory access. But the total size of globally usable registers is 
fixed, and will decrease the degree of parallelism if the per \TB usage is high.
Shared memory is also a fast and restricted resource compared to global memory.
The access to the global memory is generally the slowest and dependent on the 
access pattern. A coalesced access can help achieve higher throughput
~\cite{kepler-tuning}. 

\subsection{Coalesced memory access}

For example, according to GPU's SIMD pattern, when threads in a \TB are each 
handling the computation over one single data point and tranverse over coordinates,
a column-major storage for the point matrix will enable a more coalesced global
memory access than its row-major counterpart. Therefore, we do matrix transposition
accroding to each matrix's access pattern.

Especially, in Algorithm~\ref{par_m} and ~\ref{par_vn_m}, we compute the inner 
product of the cluster centroid matrix $c$ and transposition on point matrix
$p^T$ to get a transposed version of $p \cdot c^T$, which causes coalesced memory
access in a later computational kernel without an extra matrix transposition.

\begin{align*}
  (p \cdot c^T)^T &= c \cdot p^T
\end{align*}

\subsection{High-bandwidth memory}

We load the cluster centroids into shared memory. Since centroids are sharedly 
used by computations of all threads in the kernel, spending the highly restricted
shared memory resource on them can be most beneficial. 
But for larger number of clusters with high dimensions, the shared memory
can still be not enough. In this case,
we do iterative computations over sharded input data.
We did not use the context and texture memory in this 
implemetation considering their low flexibility.

\subsection{Prefetching}

Modern C compilers like \gcc and \clang often do prefetching automatically while
compilation. CUDA platform, on the otherhand, chooses to expose more low level 
details, and rely on the developers for deciding the data access patterns. As a 
result, the CUDA C compiler is not as advanced as C compilers in optimization.
Therefore, we do manual prefetching.
This especially happens in the \textbf{dist\_square()} function in 
Algorithm~\ref{par}. In one iteration before a point coordinate is actually used, 
we load it into a thread-private register, thus overlapping loading with 
computation.
