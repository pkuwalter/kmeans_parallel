\section{Parallel k-means clustering}
An obvious way to parallelize the algorithm is to parallelize the
parts for membership assignment and recomputation on new centroids.

There are two ways to compute distance between point $p(i)$
and centroid $c(j)$. Suppose $p(i) = (p_{i1}, p_{i2}, ..., p_{id})$ and $c(j) = (c_{j1},
c_{j2}, ..., c_{jd})$, the squared Euclidean distance between point p(i) and centroid c(j): 
\begin{align}
d(p(i),c(j)) &= (p_{i1} - c_{j1})^2 + (p_{i2} - c_{j2})^2 + ... + (p_{id} - c_{jd})^2 \label{e1}\\
&= (p_{i1}^2 + p_{i2}^2 + ... + p_{id}^2) - (2*p_{i1}*c_{j1} + 2*p_{i2}*c_{j2} + ... + 2*p_{id}*c_{jd})
+ (c_{j1}^2 + c_{j2}^2 + ... + c_{jd}^2) \label{e2}
\end{align}

\subsection{Pure CUDA implementation}
Algorithm implements \ref{e1} is listed in Algorithm~\ref{par}. 
\begin{algorithm}[!h]
  \caption{Parallel k-means clustering} \label{par}
  \begin{algorithmic}[1]
    \INPUT $K$: Number of clusters; $N$: number of d-dimensional data points; $p$: data points.
    \Function{par\_k-means}{$p, N, K$} \label{alg:p}
    \State Randomly choose $K$ points as cluster centroids $c[]$
    \While {! termination\_condition}
    \ParFor {i = 1..N}
    \For {j = 1..K}
    \For {dd = 1..d}
    \State $distance += (p(i)(dd) - c(j)(dd))^2$
    \EndFor
    \EndFor
    \State Find the nearest centroid $c_{nearest}$ for $p(i)$
    \State Change membership of $p(i)$ to the cluster with $c_{nearest}$
    \State Accumulate $p(i)$'s coordinates to the cluster's new centroid
    \EndParFor
    \State Compute new $c[]$: divide the accumulated coords by num\_points
    \State Recalculate termination condition
    \EndWhile
    \EndFunction  
  \end{algorithmic}
\end{algorithm}
Suppose m is the number of iterations in a run. The work-depth model for this algorithm is:
Work = O(NKdm), Depth = O(Kdm).

\subsection{Parallel K-means implementation using matrix operation}
\ref{e2} can be structured as vector norm of each row of matrix (norm of p(i) and c(j)) and
a matrix-matrix multiplication (p * c). The algorithm is shown in~\ref{par_m}
\begin{algorithm}[!h]
  \caption{Parallel k-means clustering using matrix operation} \label{par_m}
  \begin{algorithmic}[1]
    \INPUT $K$: Number of clusters; $N$: number of d-dimensional data points; $p$: data points.
    \Function{par\_k-means-matrix}{$p, N, K$} \label{alg:pm}
    \State Randomly choose $K$ points as cluster centroids $c[]$
    \While {! termination\_condition}
    \For {i = 1..N}
    \State $p\_norm(i) = norm(p(i))$
    \EndFor
    \For {j = 1..K}
    \State $c\_norm(j) = norm(c(j))$
    \EndFor
    \State $pc\_product = 2 .* p' * c$
    \ParFor {i = 1..N}
    \For {j = 1..K}
    \State $distance = p\_norm(i) + c\_norm(j) - pc\_product(i)(j)$
    \EndFor
    \State Find the nearest centroid $c_{nearest}$ for $p(i)$
    \State Change membership of $p(i)$ to the cluster with $c_{nearest}$
    \State Accumulate $p(i)$'s coordinates to the cluster's new centroid
    \EndParFor
    \State Compute new $c[]$: divide the accumulated coords by num\_points
    \State Recalculate termination condition
    \EndWhile
    \EndFunction
  \end{algorithmic}
\end{algorithm}
The work-depth model for this algorithm is: Work = O(2Ndm + Kdm + NKdm + NKm), Depth = O($D(norm(d))*N*m + (norm(d))*K*m + D(matrix product(NKd))*m + Km + dm$).

We implement algorithm~\ref{par_m} using cublasSgemm for matrix matrix multiplication and cublasSnrm2
for vector norm and evaluate the performance of this algorithm.
We find that calculating the vector norm using cublasSnrm2 is pretty slow while matrix-matrix multiplication is really fast.

\subsection{Computing vector norm using matrix-matrix multiplication}
Given that there are no provided function to calculate the
norm of each row of a matrix simultaneous, we come up with an idea to implement vector norm using matrix-
matrix multiplication. For a matrix A, the norm of each row of matrix is at the diagonal of A * A'. The
algorithm is shown in~\ref{par_vn_m}. 
\begin{algorithm}[!htp]
  \caption{Parallel k-means clustering using matrix operation with vector norm calculating using
    matrix-matrix multiplication} \label{par_vn_m}
  \begin{algorithmic}[1]
    \INPUT $K$: Number of clusters; $N$: number of d-dimensional data points; $p$: data points.
    \Function{par\_k-means-matrix-v2}{$p, N, K$} \label{alg:pm2}
    \State Randomly choose $K$ points as cluster centroids $c[]$
    \While {! termination\_condition}
    \State $p\_norm = diag(p * p')$
    \State $c\_norm = diag(c * c')$
    \State $pc\_product = 2 .* p' * c$
    \ParFor {i = 1..N}
    \For {j = 1..K}
    \State $distance = p\_norm(i) + c\_norm(j) - pc\_product(i)(j)$
    \EndFor
    \State Find the nearest centroid $c_{nearest}$ for $p(i)$
    \State Change membership of $p(i)$ to the cluster with $c_{nearest}$
    \State Accumulate $p(i)$'s coordinates to the cluster's new centroid
    \EndParFor
    \State Compute new $c[]$: divide the accumulated coords by num\_points
    \State Recalculate termination condition
    \EndWhile
    \EndFunction
  \end{algorithmic}
\end{algorithm}
The work depth model for this algorithm is: Work = O($N^2dm + K^2dm + NKdm + NKm$),
Depth = O($D(matrix product(N^2d))*m + D(matrix product(K^2d)*m) + D(matrix product(NKd))*m+ Km + dm$).
We do not know how cuBlas implements matrix-matrix multiplication and vector norm, so we put place holder
in depth for algorithm~\ref{par_m}\ref{par_vn_m}. 
